<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Recurrent neural networks basics</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">
        <style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
                    <section>
                        <div class="column">
                            <img src="images/jetbrains_logo_icon_145150.webp" alt="jetbrains_logo" />
                        </div>
                        <div class="column">
                            <h2>Youth AI club</h2>
                            <div class="fragment" style="margin-bottom:20px;">
                                    <div class="typesetting">
                                        <h3>Recurrent neural networks basics</h3>
                                        <br />
                                        Alex Avdiushenko <br />
                                        November 20, 2024
                                    </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">In the last episode</h3>
                        <div class="fragment" style="text-align: left">
                            <div class="typesetting">
                                <ul>
                                    <li><span class="important">Expressive power</span>
                                        of neural networks</li>
                                    <li>Demo in <span class="important">PyTorch</span></li>
                                    <li>Cross-entropy loss for binary classification</li>
                                    <li>Backpropagation algorithm</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Disadvantages of Feed-Forward (and also Convolutional) Neural Networks</h3>
                        <h4 style="text-align: left">Or why we need recurrent ones :)</h4>
                            <ul>
                                <li class="fragment">The input is only fixed-dimensional vectors (e.g., 28×28 images)</li>
                                <li class="fragment">The output is also a fixed dimension (for example, probabilities of 1000 classes)</li>
                                <li class="fragment">Fixed number of computational steps (i.e., network architecture)</li>
                            </ul>

                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left"><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">A. Karpathy. The Unreasonable Effectiveness of Recurrent Neural Networks</a></p>
                            </div>
                            <img src="../ml_fundament/images/rnn_architectures.jpg" alt="RNN Architectures" style="width:60vw; object-fit:contain;">
                        </div>

                        <aside class="notes">
                            So, how to eliminate all these disadvantages?
                            There are several different architectures. Let's look at them one by one.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">Architectures of Recurrent Networks</h3>
                        <div style="text-align:center">
                            <table style="width: 100%;">
                                <tr>
                                    <td style="text-align:center;">
                                        <img src="../ml_fundament/images/one-to-one.jpg" alt="One-to-One Architecture" style="width:14vw; object-fit:contain;">
                                    </td>
                                    <td style="text-align:center;">
                                        <img src="../ml_fundament/images/one-to-many.jpg" alt="One-to-Many Architecture" style="width:20vw; object-fit:contain;">
                                    </td>
                                </tr>
                                <tr>
                                    <td style="text-align:center;">Vanilla Neural Networks</td>
                                    <td style="text-align:center;">Image Captioning<br>image → (sequence of words)</td>
                                </tr>
                            </table>
                        </div>
                    </section>
                    <section>
                        <div style="text-align:center">
                            <table style="width: 100%;">
                                <tr>
                                    <td style="text-align:center;">
                                        <img src="../ml_fundament/images/many-to-one.jpg" alt="Many-to-One Architecture" style="width:22vw; object-fit:contain;">
                                    </td>
                                    <td style="text-align:center;">
                                        <img src="../ml_fundament/images/many-to-many.jpg" alt="Many-to-Many Architecture" style="width:36vw; object-fit:contain;">
                                    </td>
                                </tr>
                                <tr>
                                    <td style="text-align:center;">Sentiment Classification<br>(sequence of words) → sentiment</td>
                                    <td style="text-align:center;">Machine Translation<br>(seq of words) → (seq of words)</td>
                                </tr>
                            </table>
                        </div>
                    </section>
                    <section>
                        <div style="text-align:center">
                            <img src="../ml_fundament/images/many-to-many-2.jpg" alt="Many-to-Many Architecture 2" style="width:24vw; object-fit:contain;">
                            <h3>Video Classification</h3>
                            <p>(on frame level)</p>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Sequential Processing of Fixed Input</h3>
                        <div style="text-align:center">
                            <img src="../ml_fundament/images/house_read.gif" alt="Sequential Processing" style="width:22vw; object-fit:contain;">
                        </div>
                        <p>
                            <a href="https://arxiv.org/abs/1412.7755">J. Ba, V. Mnih, K. Kavukcuoglu. Multiple Object Recognition with Visual Attention</a>
                        </p>
                        <aside class="notes">
                            The interesting fact is that different recurrent architectures have also been successfully applied
                            to problems with fixed input, such as object recognition, for example.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">Sequential Generation of Fixed Output</h3>
                        <div style="text-align:center">
                            <img src="../ml_fundament/images/house_generate.gif" alt="Sequential Generation" style="width:40vw; object-fit:contain;">
                        </div>
                        <p>
                            <a href="https://arxiv.org/abs/1502.04623">K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, D. Wierstra.
                                <br>DRAW: A Recurrent Neural Network For Image Generation</a>
                        </p>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Recurrent Neural Network scheme</h3>
                        <img src="../ml_fundament/images/rnn_scheme.png" alt="rnn_scheme" width="50%">
                        <aside class="notes">
                            Ok, so what is inside the recurrent network, how this model works?
                            You have some iteration process, where on each step one and the same function changes the hidden vector of the model.
                        </aside>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Recurrent Neural Network</h3>
                        <p>We process the sequence of vectors $x$ with <strong>one and the same</strong> function with parameters:</p>
                            $$ h_t = f_W(h_{t-1}, x_t)$$
                        <p style="text-align: left">$f_W$ is a function parameterized by $W$</p>
                        <p style="text-align: left">$x_t$ — next input vector</p>
                        <p style="text-align: left">$h_t$ — hidden state</p>
                        <div class="fragment" style="margin-top:20px;">
                            <div class="typesetting">
                                <div class="r-frame important">
                                    <p>Question: What function can we take as $f_W$?</p>
                                </div>
                            </div>
                        </div>
                        <aside class="notes">
                            Which function, one of the simplest, do you know from machine learning?
                        </aside>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Vanilla Recurrent Neural Network</h3>
                            $$ h_t = f_W(h_{t-1}, x_t)$$
                        <p>As a function $f_W$ we set a linear transformation with a non-linear component-wise "sigmoid":</p>
                            $$
                            \begin{align*}
                                h_t &= \tanh ({\color{orange}W_{hh}} h_{t-1} + {\color{orange}W_{xh}} x_t) \\
                                y_t &= {\color{orange}W_{hy}} h_t
                            \end{align*}
                            $$
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Character Level Model Example</h3>
                            <p>The entire four-letter dictionary: <strong>[h, e, l, o]</strong> and word "hello" as train:</p>
                        <div style="text-align:center">
                            <img src="../ml_fundament/images/rnn_char_level_example.jpg" alt="Character Level Model Example" style="width:45vw; object-fit:contain;">
                        </div>
                        <p><strong>Softmax</strong> is also applied to the values of the out-layer to get the loss</p>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Demo</h3>
                        <p><a href="https://gist.github.com/karpathy/d4dee566867f8291f086"><strong>numpy</strong> implementation</a> by Karpathy</p>
                        <br><br>
                        <p>Let's get to grips with the code in <a href="https://colab.research.google.com/github/avalur/avalur.github.io/blob/master/ml_fundament/rnn_demo.ipynb">
                            the Jupyter notebook!</a></p>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">How does it work?</h3>
                        <div style="text-align:center">
                            <img src="../ml_fundament/images/url_neuron.png" alt="Neuron Analysis" style="width:85vw; object-fit:contain;">
                        </div>
                        <aside class="notes">
                            In his work on the analysis of recurrent networks, Andrej Karpathy tried to understand the mechanisms of this model.
                        </aside>
                    </section>
                    <section data-background-color="white">
                        <div style="text-align:center">
                            <img src="../ml_fundament/images/bracket_neuron.png" alt="Bracket Neuron" style="width:85vw; object-fit:contain;">
                        </div>
                    </section>
                    <section data-background-color="white">
                        <div style="text-align:center">
                            <img src="../ml_fundament/images/cell_endline.png" alt="Cell Endline" style="width:85vw; object-fit:contain;">
                        </div>
                        <aside class="notes">
                            Note: You see, he discovered fascinating facts about RNN mechanics and showed it well with specific examples.
                        </aside>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Deep Recurrent Networks</h3>
                        <div class="row">
                            <div class="column" style="width: 40%;">
                                <br><br>
                                $$
                                    \quad h_t^\ell = \tanh {\color{orange}W^\ell} \left(\begin{array}{c}
                                        h_t^{\ell-1} \\ h_{t-1}^{\ell}
                                    \end{array}\right)
                                $$
                                $$
                                    \quad h \in \mathbb{R}^n, \quad {\color{orange}W^\ell} [n \times 2n]
                                $$
                            </div>
                            <div class="column" style="width: 40%; text-align:center;">
                                <img src="../ml_fundament/images/rnn_depth.jpg" alt="Deep RNN" style="width:35vw; object-fit:contain;">
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="r-frame important">
                                Question: What is the main problem with vanilla RNN?
                            </div>
                        </div>
                        <aside class="notes">
                            Let's go on and talk about deep RNN with several layers.
                            So you simply add new layers in depth.

                            It is difficult for RNN to remember a long context.
                        </aside>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Long short-term memory (LSTM)</h3>
                        $$
                            \begin{align*}
                                &{\color{orange}W^\ell} [4n\times 2n] \\
                                \left(\begin{array}{c}
                                i \\ f \\ o \\ c_t^\prime
                                \end{array}\right) &=
                                \left(\begin{array}{c}
                                \text{sigm} \\ \text{sigm} \\ \text{sigm} \\ \tanh
                                \end{array}\right)
                                {\color{orange}W^\ell}
                                \left(\begin{array}{c}
                                h_t^{\ell-1} \\ h_{t-1}^{\ell}
                                \end{array}\right) \\
                                &\begin{array}{l}
                                c_t^\ell = f \odot c_{t-1}^\ell + i \odot c_t^\prime \\
                                h_t^\ell = o \odot \tanh(c_t^\ell)
                                \end{array}
                            \end{align*}
                        $$
                        <p style="text-align: center;">$\odot$ — component-wise product</p>

                        <aside class="notes">
                            LSTM modules come to the rescue! Now you have three gates and a new vector in the model — cell state.
                        </aside>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">LSTM: Motivation and Schema</h3>
                        <p style="text-align: left">The network should remember the context for a long time. Which context? The network learns itself.
                           To do this, the vector \(c_t\) is introduced, which is the state vector of the network at the moment \(t\).
                        </p>
                        <div>
                            <table style="width:100%;">
                                <tr>
                                    <td>\(c_t^\prime = \tanh({\color{orange}W_{xc}}x_t + {\color{orange}W_{hc}}h_{t-1} + {\color{orange}b_{c^\prime}})\)</td>
                                    <td>candidate cell state</td>
                                </tr>
                                <tr>
                                    <td>\(i_t = \sigma({\color{orange}W_{xi}}x_t + {\color{orange}W_{hi}}h_{t-1} + {\color{orange}b_{i}})\)</td>
                                    <td>input gate</td>
                                </tr>
                                <tr>
                                    <td>\(f_t = \sigma({\color{orange}W_{xf}}x_t + {\color{orange}W_{hf}}h_{t-1} + {\color{orange}b_{f}})\)</td>
                                    <td>forget gate</td>
                                </tr>
                                <tr>
                                    <td>\(o_t = \sigma({\color{orange}W_{xo}}x_t + {\color{orange}W_{ho}}h_{t-1} + {\color{orange}b_{o}})\)</td>
                                    <td>output gate</td>
                                </tr>
                                <tr>
                                    <td>\(c_t = f_t \odot c_{t-1} + i_t \odot c_t^\prime\)</td>
                                    <td>cell state</td>
                                </tr>
                                <tr>
                                    <td>\(h_t = o_t \odot \tanh(c_t)\)</td>
                                    <td>block output</td>
                                </tr>
                            </table>
                        </div>
                    </section>

                    <section data-background-color="white">
                        <img src="../ml_fundament/images/lstm_scheme.png" alt="lstm_scheme" width="60%">
                    </section>

                    <section data-background-color="white">
                        <h3 style="text-align: left">GRU: Gated Recurrent Unit</h3>
                        $$
                            \begin{align*}
                                u_t &= \sigma({\color{orange}W_{xu}}x_t + {\color{orange}W_{hu}}h_{t-1} + {\color{orange}b_{u}}) \\
                                r_t &= \sigma({\color{orange}W_{xr}}x_t + {\color{orange}W_{hr}}h_{t-1} + {\color{orange}b_{r}}) \\
                                h_t^\prime &= \tanh({\color{orange}W_{xh^\prime}}x_t + {\color{orange}W_{hh^\prime}}(r_t\odot h_{t-1})) \\
                                h_t &= (1-u_t) \odot h_t^\prime + u_t \odot h_{t-1}
                            \end{align*}
                        $$
                        <p style="text-align: left">Only \( h_t \) is used, vector \( c_t \) is not introduced.
                        Update-gate instead of input and forget.
                        The reset gate determines how much memory to move forward from the previous step.</p>

                        <aside class="notes">
                            And finally another one module is GRU — Gated Recurrent Unit.
                            In fact, it is more or less equivalent to LSTM, only it uses fewer gates and doesn't introduce the cell state vector \( c_t \).
                        </aside>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Disadvantages of Vanilla RNN</h3>
                            $$ h_t = f_W(h_{t-1}, x_t)$$
                        <p style="text-align: left">
                            As a function \( f_W \) we set a linear transformation with a non-linear component-wise "sigmoid":</p>
                        $$
                            \begin{align*}
                                h_t &= \tanh ({\color{orange}W_{hh}} h_{t-1} + {\color{orange}W_{xh}} x_t) \\
                                y_t &= {\color{orange}W_{hy}} h_t
                            \end{align*}
                        $$
                        <div class="fragment" style="margin-top:20px;">
                            <h4 style="text-align: left">Disadvantages</h4>
                            <ul>
                                <li>Input and output signal lengths must match</li>
                                <li>"Reads" the input only from left to right, does not look ahead</li>
                                <li>Therefore, it is not suitable for machine translation, question answering tasks, and others</li>
                            </ul>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">RNN for sequence synthesis (seq2seq)</h3>
                        <p>\(X = (x_1, \dots, x_n)\) — input sequence</p>
                        <p>\(Y = (y_1, \dots, y_m)\) — output sequence</p>
                        <p>\(\color{green}c \equiv h_n\) encodes all information about \(X\) to synthesize \(Y\)</p>
                        <div class="column">
                            <div style="text-align:center">
                                <img src="../ml_fundament/images/seq2seq.png" alt="Sequence to Sequence Model" style="width:50vw; object-fit:contain;">
                            </div>
                        </div>
                        <div class="column">
                            $$
                                \begin{align*}
                                    h_i &= f_{in}(x_i, h_{i-1}) \\
                                    {\color{green}h_t^\prime} &{\color{green}= f_{out}(h_{t-1}^\prime,y_{t-1},c)} \\
                                    y_t &= f_{y}(h_t^\prime, y_{t-1})
                                \end{align*}
                            $$
                        </div>
                    </section>
                </section>
                <section data-background-color="white">
                    <h3 style="text-align: left">Summary</h3>
                    <ul>
                        <li>Recurrent neural networks — a simple, powerful, and flexible approach to solving various machine learning problems</li>
                        <li>Vanilla RNNs are simple, but still not good enough</li>
                        <li>So you need to use LSTM or GRU, and seq2seq architecture</li>
                        <li>LSTM prevents zeroing gradients</li>
                        <li>Clipping helps with "explosion of gradients" as usual</li>
                        <li>We need a deeper understanding, both theoretical and practical — there are many open questions</li>
                    </ul>
                    <div class="fragment">
                        <p style="text-align: left">What else can you look at?</p>
                        <ul>
                            <li><a href="https://www.youtube.com/watch?v=iX5V1WpxxkY">Lecture 10</a> of the course "CS231n" by Andrej Karpathy at Stanford</li>
                            <li><a href="http://karpathy.github.io/2019/04/25/recipe/">How to</a> train neural networks?</li>
                            <li><a href="https://www.youtube.com/watch?v=_JuQcodHANs">Oct, 2022 interview</a> of Andrej Karpathy by Lex Fridman</li>
                        </ul>
                    </div>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>