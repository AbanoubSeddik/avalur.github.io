<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>After pretraining steps: RLHF, DPO and prompting</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
			ul.custom-list {
				list-style: none;
				padding: 0;
			}
			ul.custom-list li::before {
				content: '+';
				margin-right: 8px;
                color: green;
                font-size: larger;
			}
			ul.custom-list li.minus::before {
				content: '-';
                color: red;
                font-size: larger;
			}
			ul.custom-list li.lamp::before {
				content: 'ðŸ’¡';
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
					<div>
						<img src="./images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
                    <h2>Advanced machine learning</h2>
					<div class="fragment" style="margin-bottom:20px;">
							<div class="typesetting">
								<h3>After pretraining steps: RLHF, DPO and prompting</h3>
								<br />
								Alex Avdiushenko<br />
								April 16, 2025
							</div>
					</div>
                </section>
                <section>
                    <section data-background-image="images/gpt-assistant-pipeline.jpeg">
                    </section>
                    <section>
                        <h3 style="text-align: left">Instruction tuning</h3>
                        <a href="https://arxiv.org/pdf/2203.02155.pdf">
                            <img src="images/instruction_HF.png" width="80%" style="border-radius: 5%">
                        </a>
                    </section>
                    <section>
                        <h3 style="text-align: left">Scaling Instruction-Finetuned Language Models</h3>

                        <a href="https://arxiv.org/pdf/2210.11416.pdf">
                            <img src="images/scaling_FT_LM.png" width="80%" style="border-radius: 5%">
                        </a>
                    </section>
                    <section>
                        <a href="https://arxiv.org/pdf/2210.11416.pdf">
                            <img src="images/scaling_FT_LM_f2.png" width="80%" style="border-radius: 5%">
                        </a>
                    </section>
                    <section>
                        <h3 style="text-align: left">The release of LLaMA led to open-source attempts 
                            to "create" instruction tuning data</h3>
                        <img src="images/LlaMA_graph.png" alt="LlaMA_graph" width="60%" style="border-radius: 5%">
                        <p><a href="https://ai.v-gar.de/ml/transformer/timeline/llama.html">
                            https://ai.v-gar.de/ml/transformer/timeline/llama.html
                        </a></p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Limitations of Instruction Finetuning?</h3>
                        <ul>
                            <li><strong>Expensive:</strong> Collecting ground-truth data for tasks is costly</li>
                            <li><strong>Subtle Limitations, can you think of any?</strong>
                                <ul class="fragment">
                                    <li><strong class="important">Problem 1:</strong>
                                        Open-ended creative generation tasks have no right answer. <br>
                                        <em>Example:</em> "Write me a story about a dog and her pet grasshopper"</li>
                                    <li><strong class="important">Problem 2:</strong>
                                        Language modeling penalizes all token-level mistakes equally,
                                        but some errors are worse than others</li>
                                </ul>
                            </li>
                            <li class="fragment">Even with instruction finetuning,
                                there is a mismatch between the LM objective and the objective of "satisfying human preferences"!</li>
                            <li class="fragment">Can we <strong>explicitly</strong> attempt to satisfy human preferences?</li>
                        </ul>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">RLHF (Reinforcement Learning with Human Feedback)</h3>
                        <ul>
                            <li>Letâ€™s say we were training a language model on some task (e.g., summarization)</li>
                            <li>For each LM sample \(s\), imagine we had a way to obtain a
                                <strong class="important">human reward</strong> of that summary: \( R(s) \in \mathbb{R} \), where higher is better</li>
                        </ul>
                        <p class="fragment" style="margin-top: 30px;text-align: left">
                            Now we want to maximize the <strong class="important">expected reward</strong> of samples from our LM:
                            \[
                            \mathbb{E}_{\hat{s} \sim p_{\theta}(s)}\left[ R(\hat{s}) \right] \to \max_{\theta}
                            \]
                        </p>
                    </section>
                    <section>
                        <a href="https://arxiv.org/pdf/2203.02155">
                            <img src="images/instruct_GPT3.png" alt="instruct_GPT3" width="100%">
                        </a>
                    </section>
                    <section>
                        <h3 style="text-align: left">
                            <span class="important">Problem 1:</span> Human-in-the-loop is expensive</h3>
                        <p style="text-align: left"><span>Solution:</span>
                            Instead of directly asking humans for preferences,
                            <strong class="important">model their preferences</strong>
                            as a separate (NLP) problem!
                            <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/KCAP09-knox.pdf">[Knox and Stone, 2009]</a>
                        </p>
                        <div style="text-align: center;">
                            <img src="images/TAMER.png" alt="Human-in-the-Loop Framework" style="width: 45%;border-radius: 5%">
                            <p style="text-align: center;">
                                 Framework for Training an Agent Manually via Evaluative Reinforcement (TAMER)</p>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left"><span style="color: orange;">Problem 2:</span>
                            Human judgments are noisy and miscalibrated</h3>
                        <p style="text-align: left"><span>Solution:</span> Instead of asking for direct ratings,
                            ask for <strong class="important">pairwise comparisons</strong>,
                            which can be more reliable
                        <br><br>
                        <div style="text-align: center;">
                            <p style="color: orange;text-align: left"><strong>Bradley-Terry [1952]</strong> paired comparison model</p>
                                \[
                                J_{RM}(\phi) = -\mathbb{E}_{({\color{green}s^w}, {\color{red}s^l})
                                    \sim D}\left[\log \sigma(RM_{\phi}({\color{green}s^w}) - RM_{\phi}({\color{red}s^l}))\right]
                                \]
                            <p>
                                <span style="color: green;">"Winning"</span> sample \({\color{green}s^w}\)
                                should score higher than <span style="color: red;">"losing"</span> sample \({\color{red}s^l}\)
                            </p>
                        </div>
                    </section>
                    <section>
                      <h3 style="text-align: left">RLHF: Optimizing the learned reward model</h3>
                        <p style="text-align: left">We want to optimize:</p>
                        \[
                        \mathbb{E}_{\hat{y} \sim p_{\theta}^{RL}(\hat{y} | x)} [RM_{\phi}(x, \hat{y})]
                        \]
                        <p style="text-align: left">Do you see any problems?</p>
                        <div class="fragment">
                          <ul>
                            <li>Learned rewards are imperfect; this quantity can be imperfectly optimized</li>
                            <li>Add a penalty for drifting too far from the initialization:</li>
                          </ul>
                        \[
                        \mathbb{E}_{\hat{y} \sim p_{\theta}^{RL}(\hat{y} | x)} [RM_{\phi}(x, \hat{y}) - \beta \log \frac{p_{\theta}^{RL}(\hat{y} | x)}{p_{\theta}^{PT}(\hat{y} | x)}]
                        \]
                        </div>
                        <p class="fragment" style="text-align: left">This penalty prevents us from diverging too far from the pretrained model, known as the Kullback-Leibler (KL) divergence.</p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Learning to summarize from human feedback</h3>
                        <a href="https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf">
                            <img src="images/summarize_from_HF.png" alt="summarize_from_HF" width="80%" style="border-radius: 5%">
                        </a>
                    </section>
                    <section>
                        <h3 style="text-align: left">Training LM to follow instructions with HF</h3>
                        <a href="https://arxiv.org/pdf/2203.02155">
                            <img src="images/LM_to_follow_instruct.png" alt="LM_to_follow_instruct" width="70%" style="border-radius: 5%">
                        </a>
                    </section>
                    <section>
                        <img src="images/Introducing_chatGPT.png" alt="Introducing_chatGPT" width="60%">
                        <p>ChatGPT = Instruction Finetuning + RLHF for dialog agents</p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Methods</h3>
                        <p style="text-align: right"><span style="color: red;">(Instruction Finetuning!)</span></p>
                        <p style="text-align: left">We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, with slight differences in data collection. Initially, we used supervised fine-tuning, where human AI trainers provided conversations, playing both user and assistant roles.
                            We mixed this new dialogue dataset with the InstructGPT dataset, converting it into dialogue format.</p>
                        <br>
                        <p style="text-align: left">To create a reward model, we collected comparison data from conversations. We used Proximal Policy Optimization to fine-tune the model over several iterations.</p>
                        <p style="text-align: center; color: red;"><strong>(RLHF!)</strong></p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Can We Simplify RLHF? Towards <span style="color: teal;">Direct Preference Optimization</span></h3>
                        <p style="text-align: left">
                            <ul>
                                <li class="fragment">Current pipeline is as follows:
                                    <ul>
                                        <li>Train a reward model \( RM_{\phi}(x,y) \) to produce scalar rewards for LM outputs,
                                            trained on a <strong>dataset of human comparisons</strong></li>
                                        <li>Optimize pretrained (possibly instruction-finetuned) LM \( p_{\theta}^{PT}(y \mid x) \)
                                            to produce the final RLHF LM \( p_{\theta}^{RL}(\hat{y} \mid x) \)</li>
                                    </ul>
                                </li>
                                <li class="fragment">What if there was a way to write \( RM_{\phi}(x,y) \) in terms of \( p_{\theta}^{RL}(\hat{y} \mid x) \)?
                                    <ul>
                                        <li>Derive \( RM_{\theta}(x,y) \) in terms of \( p_{\theta}^{RL}(\hat{y} \mid x) \)</li>
                                        <li>Optimize parameters \(\theta\) by fitting \( RM_{\theta}(x,y) \) to the preference data instead of \( RM_{\phi}(x,y) \)</li>
                                    </ul>
                                </li>
                                <li class="fragment">How is this possible? The only external information to the optimization comes from the preference labels</li>
                            </ul>
                        </p>
                    </section>
                    <section>
                      <h3 style="text-align: left">Direct Preference Optimization (DPO)</h3>
                      <ul>
                        <li>Recall how we fit the reward model \( RM_{\phi}(x, y) \):</li>
                        <div class="fragment">
                          \[
                          J_{RM}(\phi) = -\mathbb{E}_{(x, {\color{green}y^w}, {\color{red}y^l}) \sim D}[\log
                            \sigma(RM_{\phi}(x, {\color{green}y^w}) - RM_{\phi}(x, {\color{red}y^l}))]
                          \]
                        </div>
                        <li class="fragment">Notice that we only need the difference between the rewards
                            for \( {\color{green}y^w} \) and \( {\color{red}y^l} \).
                            Simplify for \( RM_{\theta}(x, y) \):</li>
                        <div class="fragment">
                          \[
                          RM_{\theta}(x, {\color{green}y^w}) - RM_{\theta}(x, {\color{red}y^l}) =
                            \beta \log \left(\frac{p_{\theta}^{RL}({\color{red}y^l} | x)}{p_{\theta}^{PT}({\color{red}y^l} | x)}\right) -
                            \beta \log \left(\frac{p_{\theta}^{RL}({\color{green}y^w} | x)}{p_{\theta}^{PT}({\color{green}y^w} | x)}\right)
                          \]
                        </div>
                        <li class="fragment">The final DPO loss function is:</li>
                        <div class="fragment">
                          \[
                          J_{DPO}(\theta) = -\mathbb{E}_{(x, {\color{green}y^w}, {\color{red}y^l}) \sim D}
                            \left[ \log
                            \sigma \left(RM_{\theta}(x, {\color{green}y^w}) - RM_{\theta}(x, {\color{red}y^l})\right)\right]
                          \]
                        </div>
                        <p class="fragment r-frame">We have a <em>simple classification loss function</em> that connects preference data to language model parameters directly!</p>
                      </ul>
                    </section>
                    <section>
                        <h3 style="text-align: left">DPO results</h3>
                        <img src="images/DPO_results.png" alt="DPO_results" width="80%">
                        <p>Source: <a href="https://web.stanford.edu/class/cs224n/slides/cs224n-spr2024-lecture10-prompting-rlhf.pdf">
                            Stanford class cs224n, lecture10 post-training</a></p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Summary (DPO and RLHF)</h3>
                        <ul>
                            <li>We aim to optimize for human preferences by ranking
                                LM-generated answers instead of direct writing or scoring</li>
                            <li class="fragment">Reinforcement Learning from Human Feedback (RLHF):
                                <ul>
                                    <li>Train a reward model using comparison data to predict scores for completions</li>
                                    <li>Optimize the LM to maximize these scores (with KL-constraint)</li>
                                    <li>Effective but computationally challenging</li>
                                </ul>
                            </li>
                            <li class="fragment">Direct Preference Optimization (DPO):
                                <ul>
                                    <li>Optimize LM parameters directly using binary classification</li>
                                    <li>Simple, effective, similar to RLHF, but without online data use</li>
                                </ul>
                            </li>
                        </ul>
                    </section>
                </section>
                <section>
                    <section>
                      <h3 style="text-align: left">Prompting as a Prior</h3>
                        <p style="text-align: left">
                            Prompts are a human-interpretable method for providing a prior over the parameters of the model!
                        </p>
                        <p style="text-align: left">
                            This can be used in place of fine-tuning, or combined with fine-tuning.
                        </p>
                        <br>
                        <p style="text-align: left">
                            <a href="https://arxiv.org/pdf/2001.07676.pdf">https://arxiv.org/pdf/2001.07676.pdf</a>
                        </p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Behind the scenes, messages are converted to token strings</h3>
                        <div style="display: flex; justify-content: space-around;">
                            <div>
                                <h4>LLaMa</h4>
                                <pre style="background-color: gray; padding: 10px;">
Sys. [INST]
<< SYS >>
You are an assistant that â€¦
<< SYS >>
[/INST]

User [INST]This movie is great.[/INST]

Asst. Positive.
                                </pre>
                            </div>
                            <div>
                                <h4>Alpaca</h4>
                                <pre style="background-color: gray; padding: 10px;">
Sys. ### Instruction:
You are an assistant that â€¦

User ### Instruction:
This movie is great.

Asst. ### Response:
Positive.
                                </pre>
                            </div>
                        </div>
                    </section>
                    <section>
                      <h3 style="text-align: left">Post-processing of answer</h3>
                      <ul>
                        <li>Based on the answer, select the actual output</li>
                        <li>For instance:
                          <ul>
                            <li>Taking the output as-is</li>
                            <li>Formatting the output for easy visualization</li>
                            <li>Selecting only parts of the output that you want to use</li>
                            <li>Mapping the outputs to other actions</li>
                          </ul>
                        </li>
                      </ul>
                      <img src="images/post_proc_example.png" alt="post_proc_example" width="70%"
                      style="border-radius: 5%">
                    </section>
                    <section>
                        <h3 style="text-align: left">Chain-of-Thought Prompting</h3>
                        <p style="text-align: left">Chain-of-Thought (CoT) prompting is a method
                            for generating prompts that guide the model to produce more coherent and
                            focused responses</p>
                        <p><a href="https://arxiv.org/pdf/2201.11903.pdf">
                        <img src="images/CoT_prompting.png" alt="CoT_prompting" style="border-radius: 5%">
                        </a></p>
                    </section>
                    <section>
                        <img src="images/LM_designed_prompt.png" alt="LM_designed_prompt" style="border-radius: 5%">
                        <p>
                            <a href="https://arxiv.org/pdf/2211.01910">
                                Large language models are human-level prompt engineers
                            </a>
                        </p>
                    </section>
                    <section>
                      <h3 style="text-align: left">Downside of prompt-based learning</h3>
                      <ol>
                        <li><strong>Inefficiency:</strong> The prompt needs to be processed <em>every time</em>
                            the model makes a prediction</li>
                        <li><strong>Poor performance:</strong> Prompting generally performs worse than fine-tuning
                            <a href="https://arxiv.org/abs/2005.14165">[Brown et al., 2020]</a></li>
                        <li><strong>Sensitivity</strong> to the wording of the prompt,
                            order of examples, etc.</li>
                        <li><strong>Lack of clarity</strong> regarding what the model learns from the prompt.
                            Even random labels work <a href="https://arxiv.org/abs/2307.12375">[Kossen et al., 2023]</a>!</li>
                      </ol>
                    </section>
                </section>
                <section>
                    <section>
                      <h3>From Language Models to Assistants</h3>
                      <ol>
                        <li class="fragment">
                          <strong>Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning</strong>
                          <ul class="custom-list">
                            <li>No finetuning needed, prompt engineering (e.g. CoT) can improve performance</li>
                            <li class="minus">Limits to what you can fit in context</li>
                            <li class="minus">Complex tasks will probably need gradient steps</li>
                          </ul>
                        </li>
                        <li class="fragment">
                          <strong>Instruction finetuning</strong>
                          <ul class="custom-list">
                            <li>Simple and straightforward, generalizes to unseen tasks</li>
                            <li class="minus">Collecting demonstrations for so many tasks is expensive</li>
                            <li class="minus">Mismatch between LM objective and human preferences</li>
                          </ul>
                        </li>
                        <li class="fragment">
                          <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>
                          <ul class="custom-list">
                            <li>Directly model preferences (cf. language modeling), generalizes beyond labeled data</li>
                            <li class="minus">RL is very tricky to get right</li>
                            <li class="minus">Human preferences are fallible; models of human preferences even more so</li>
                          </ul>
                        </li>
                      </ol>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>