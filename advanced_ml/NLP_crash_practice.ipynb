{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic modeling and word2vec\n",
    "\n",
    "This notebook examines two topic modeling models from the `gensim` library:\n",
    "  - LDA (Latent Dirichlet Allocation)\n",
    "  - word2vec\n",
    "\n",
    "Sources of inspiration:\n",
    "  - https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
    "  - https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0fw1oPDs6wY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA (Latent Dirichlet Allocation)\n",
    "We install the topic modeling library gensim (http://radimrehurek.com/gensim/) and load the NLTK library (http://nltk.org/), which will be needed for lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-08-21T07:12:54.014181Z",
     "start_time": "2024-08-21T07:12:25.115997Z"
    }
   },
   "source": [
    "!pip install --upgrade gensim\n",
    "!pip install --upgrade nltk"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\r\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/2a/15/aca2fc3b9e97bd0e28be4a4302793c43757b04b828223c6d103c72132f19/gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/Aleksandr.Avdiushenko/IdeaProjects/alpha-zero-course/venv/lib/python3.11/site-packages (from gensim) (1.26.4)\r\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\r\n",
      "  Obtaining dependency information for scipy<1.14.0,>=1.7.0 from https://files.pythonhosted.org/packages/ba/92/42476de1af309c27710004f5cdebc27bec62c204db42e05b23a302cb0c9a/scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\r\n",
      "  Downloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (60 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.6/60.6 kB\u001B[0m \u001B[31m709.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting smart-open>=1.8.1 (from gensim)\r\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/65/12/cc24847b4b0b124501a33cd8f7963f79f6f6584bc7f2f4fc16bbbaa54c8f/smart_open-7.0.4-py3-none-any.whl.metadata\r\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\r\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\r\n",
      "  Obtaining dependency information for wrapt from https://files.pythonhosted.org/packages/0f/16/ea627d7817394db04518f62934a5de59874b587b792300991b3c347ff5e0/wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\r\n",
      "Downloading gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.0/24.0 MB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl (30.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.3/30.3 MB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached smart_open-7.0.4-py3-none-any.whl (61 kB)\r\n",
      "Downloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\r\n",
      "Installing collected packages: wrapt, scipy, smart-open, gensim\r\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1 smart-open-7.0.4 wrapt-1.16.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting nltk\r\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl.metadata\r\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: click in /Users/Aleksandr.Avdiushenko/IdeaProjects/alpha-zero-course/venv/lib/python3.11/site-packages (from nltk) (8.1.7)\r\n",
      "Collecting joblib (from nltk)\r\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl.metadata\r\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/Aleksandr.Avdiushenko/IdeaProjects/alpha-zero-course/venv/lib/python3.11/site-packages (from nltk) (2024.5.15)\r\n",
      "Requirement already satisfied: tqdm in /Users/Aleksandr.Avdiushenko/IdeaProjects/alpha-zero-course/venv/lib/python3.11/site-packages (from nltk) (4.66.4)\r\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m0m\r\n",
      "\u001B[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\r\n",
      "Installing collected packages: joblib, nltk\r\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gdyoxngvyleT",
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-08-21T07:14:46.760768Z",
     "start_time": "2024-08-21T07:14:46.243515Z"
    }
   },
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "from math import log\n",
    "from time import time\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MBeC_QT3wz3",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We read the collection of source texts into a list of documents. Each document is a list of lemmas (tokens). In this example, we load the entire collection into memory. In fact, `gensim` allows you to avoid this at all stages of model building.\n",
    "\n",
    "The collection used is articles from the NeurIPS conference, one of the standard collections for topic modeling. The number of documents is about 1700, with each document having a length of 1000-2000 words. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNi0iXA-1WS1",
    "outputId": "cf89826a-16a6-4100-fd10-4c10151eb683",
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-08-21T07:16:03.670039Z",
     "start_time": "2024-08-21T07:15:56.066022Z"
    }
   },
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request, zipfile\n",
    "\n",
    "\n",
    "tarfile_url = 'https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'\n",
    "filename = 'nips12raw_str602.tgz'\n",
    "urllib.request.urlretrieve(tarfile_url, filename)\n",
    "\n",
    "def extract_documents(fname=filename):\n",
    "    with tarfile.open(fname, mode='r:gz') as tar:\n",
    "        # Ignore directory entries, as well as files like README, etc.\n",
    "        files = [\n",
    "            m for m in tar.getmembers()\n",
    "            if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
    "        ]\n",
    "        for member in sorted(files, key=lambda x: x.name):\n",
    "            member_bytes = tar.extractfile(member).read()\n",
    "            yield member_bytes.decode('utf-8', errors='replace')\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-08-21T07:16:06.144738Z",
     "start_time": "2024-08-21T07:16:05.297151Z"
    }
   },
   "source": [
    "docs = list(extract_documents())\n",
    "print(len(docs))\n",
    "print(print(docs[0][:500]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "1 \n",
      "CONNECTIVITY VERSUS ENTROPY \n",
      "Yaser S. Abu-Mostafa \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "How does the connectivity of a neural network (number of synapses per \n",
      "neuron) relate to the complexity of the problems it can handle (measured by \n",
      "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
      "functions can be implemented using a circuit with very low connectivity (e.g., \n",
      "using two-input NAND gates). However, for a network that learns a pr\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1jWGQ6143kc"
   },
   "source": [
    "Data preparation:\n",
    "- Create a dictionary\n",
    "- Perform lemmatization\n",
    "- Build n-grams\n",
    "- Filter out tokens that are too frequent or too rare"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BPPS5jSN4Fy9",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:17:09.888149Z",
     "start_time": "2024-08-21T07:17:09.134925Z"
    }
   },
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words."
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-kzrHz65gQh",
    "outputId": "fbb1e8cf-3b42-4c95-f75c-deef3efad1d0",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:17:21.649318Z",
     "start_time": "2024-08-21T07:17:21.472858Z"
    }
   },
   "source": [
    "print(np.sum([len(doc) for doc in docs]))\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "print(np.sum([len(doc) for doc in docs]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5461201\n",
      "5115888\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T07:17:26.481977Z",
     "start_time": "2024-08-21T07:17:26.479983Z"
    }
   },
   "source": [
    "print(docs[1][:50])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stochastic', 'learning', 'networks', 'and', 'their', 'electronic', 'implementation', 'joshua', 'alspector', 'robert', 'b', 'allen', 'victor', 'hut', 'and', 'srinagesh', 'satyanarayana', 'bell', 'communications', 'research', 'morristown', 'nj', 'abstract', 'we', 'describe', 'a', 'family', 'of', 'learning', 'algorithms', 'that', 'operate', 'on', 'a', 'recurrent', 'symmetrically', 'connected', 'neuromorphic', 'network', 'that', 'like', 'the', 'boltzmann', 'machine', 'settles', 'in', 'the', 'presence', 'of', 'noise']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQkmqQQ97bIj",
    "outputId": "65967b8f-6af2-462f-b550-525e07f82452",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:17:46.852878Z",
     "start_time": "2024-08-21T07:17:46.752670Z"
    }
   },
   "source": [
    "# Remove words that are only one character\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "print(np.sum([len(doc) for doc in docs]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4629808\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSjycOUfGhCw",
    "outputId": "7f6d019c-6f18-4cf3-c6ae-d91cbf923185",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:18:30.408726Z",
     "start_time": "2024-08-21T07:18:30.288796Z"
    }
   },
   "source": [
    "# Remove words with underscores, since we are going to use them as delimiters in bigrams\n",
    "docs = [[token for token in doc if '_' not in token] for doc in docs]\n",
    "print(np.sum([len(doc) for doc in docs]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4626035\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HUBuhfJNERyR",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:18:43.342952Z",
     "start_time": "2024-08-21T07:18:38.675617Z"
    }
   },
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Aleksandr.Avdiushenko/nltk_data...\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtGLu8yaEkvU",
    "outputId": "77dca455-dd2a-4cbe-f17b-4a5759feaa57",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:19:27.206286Z",
     "start_time": "2024-08-21T07:19:27.204289Z"
    }
   },
   "source": [
    "print(lemmatizer.lemmatize('abstracts'),\n",
    "      lemmatizer.lemmatize('fishes'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract fish\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fCkF1Hqm50BK",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:19:43.292030Z",
     "start_time": "2024-08-21T07:19:36.065915Z"
    }
   },
   "source": [
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mdyb94Br6OZh",
    "outputId": "792d541f-af70-45a3-bcc5-4e37b99add50",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:19:48.118939Z",
     "start_time": "2024-08-21T07:19:45.716247Z"
    }
   },
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 10:19:45,717 : INFO : collecting all words and their counts\n",
      "2024-08-21 10:19:45,717 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2024-08-21 10:19:48,117 : INFO : collected 1114271 token types (unigram + bigrams) from a corpus of 4626035 words and 1740 sentences\n",
      "2024-08-21 10:19:48,117 : INFO : merged Phrases<1114271 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n",
      "2024-08-21 10:19:48,117 : INFO : Phrases lifecycle event {'msg': 'built Phrases<1114271 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000> in 2.40s', 'datetime': '2024-08-21T10:19:48.117865', 'gensim': '4.3.3', 'python': '3.11.9 (main, Jul  9 2024, 11:54:13) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iT1DIBdvG3jq",
    "outputId": "ce101124-b0c0-49b7-e295-ae4ba21c8ac7",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:20:40.870616Z",
     "start_time": "2024-08-21T07:20:40.868202Z"
    }
   },
   "source": [
    "for token in bigram[docs[0][:100]]:\n",
    "    if '_' in token:\n",
    "        print(token)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abu_mostafa\n",
      "california_institute\n",
      "technology_pasadena\n",
      "ca_abstract\n",
      "neural_network\n",
      "boolean_function\n",
      "can_be\n",
      "very_low\n",
      "learning_rule\n",
      "lower_bound\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBnDw_SwHJvv",
    "outputId": "668b3088-fd02-470c-b612-20b7f277664c",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:22:38.531891Z",
     "start_time": "2024-08-21T07:22:35.039131Z"
    }
   },
   "source": [
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document\n",
    "            docs[idx].append(token)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = Dictionary(docs)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove words that are too rare (e.g., typos) and words that are too frequent (e.g., stop words or just common non-topic terms). The `filter_extremes` function removes tokens from the dictionary that appear in less than `no_below` documents or in more than `no_above` fraction of the total number of documents."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove rare and common tokens\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Represent all documents in vector form (Bag-of-Words)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GH6NNGAKH5OD",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:25:20.663217Z",
     "start_time": "2024-08-21T07:25:20.058681Z"
    }
   },
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yKG6JJ9Ili4",
    "outputId": "fc7dab88-81ef-4cc9-e96e-921fa7499252",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:25:25.692248Z",
     "start_time": "2024-08-21T07:25:25.690228Z"
    }
   },
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 8623\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T07:25:33.638350Z",
     "start_time": "2024-08-21T07:25:33.636008Z"
    }
   },
   "cell_type": "code",
   "source": "print(corpus[0][:10])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4), (1, 1), (2, 1), (3, 2), (4, 4), (5, 4), (6, 1), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGMtwCDqxg8N"
   },
   "source": [
    "### Training\n",
    "Now we are ready to build a topic model for our collection. We will build an online LDA model, implemented in the `gensim` library. We specify the vectorized corpus of texts, the dictionary, and the number of topics (10). We will discuss the remaining parameters later."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGrywSb9MvMK",
    "outputId": "cc0be619-05ae-46ca-8546-77cee2305076",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:28:10.088865Z",
     "start_time": "2024-08-21T07:27:54.961429Z"
    }
   },
   "source": [
    "start = time()\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000  # batch-size\n",
    "epochs = 5   \n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=epochs,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "print('Evaluation time: {}'.format((time()-start) / 60))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 10:27:54,963 : INFO : using autotuned alpha, starting with [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "2024-08-21 10:27:54,965 : INFO : using serial LDA version on this node\n",
      "2024-08-21 10:27:54,971 : INFO : running online (multi-pass) LDA training, 10 topics, 5 passes over the supplied corpus of 1740 documents, updating model once every 1740 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2024-08-21 10:27:54,972 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2024-08-21 10:27:54,972 : INFO : PROGRESS: pass 0, at document #1740/1740\n",
      "2024-08-21 10:28:00,293 : INFO : optimized alpha [0.09430594, 0.071321055, 0.08636341, 0.07950668, 0.07086547, 0.06970359, 0.084672704, 0.074144214, 0.051429193, 0.08029875]\n",
      "2024-08-21 10:28:00,297 : INFO : topic #8 (0.051): 0.004*\"recognition\" + 0.003*\"speech\" + 0.003*\"noise\" + 0.003*\"class\" + 0.003*\"sequence\" + 0.003*\"mixture\" + 0.003*\"component\" + 0.003*\"source\" + 0.003*\"signal\" + 0.003*\"frequency\"\n",
      "2024-08-21 10:28:00,297 : INFO : topic #5 (0.070): 0.007*\"rule\" + 0.005*\"neuron\" + 0.005*\"control\" + 0.004*\"action\" + 0.003*\"cell\" + 0.003*\"noise\" + 0.003*\"dynamic\" + 0.003*\"signal\" + 0.003*\"estimate\" + 0.003*\"optimal\"\n",
      "2024-08-21 10:28:00,297 : INFO : topic #6 (0.085): 0.005*\"neuron\" + 0.003*\"hidden\" + 0.003*\"class\" + 0.003*\"classifier\" + 0.003*\"tree\" + 0.003*\"optimal\" + 0.002*\"cell\" + 0.002*\"ii\" + 0.002*\"prediction\" + 0.002*\"noise\"\n",
      "2024-08-21 10:28:00,298 : INFO : topic #2 (0.086): 0.005*\"neuron\" + 0.004*\"sample\" + 0.004*\"cell\" + 0.004*\"class\" + 0.003*\"image\" + 0.003*\"motion\" + 0.003*\"memory\" + 0.002*\"activity\" + 0.002*\"layer\" + 0.002*\"signal\"\n",
      "2024-08-21 10:28:00,298 : INFO : topic #0 (0.094): 0.004*\"image\" + 0.004*\"class\" + 0.004*\"word\" + 0.004*\"node\" + 0.004*\"recognition\" + 0.004*\"hidden\" + 0.003*\"layer\" + 0.003*\"field\" + 0.003*\"net\" + 0.003*\"object\"\n",
      "2024-08-21 10:28:00,298 : INFO : topic diff=1.159129, rho=1.000000\n",
      "2024-08-21 10:28:00,301 : INFO : PROGRESS: pass 1, at document #1740/1740\n",
      "2024-08-21 10:28:03,397 : INFO : optimized alpha [0.079929546, 0.055736262, 0.0641087, 0.06237123, 0.05984237, 0.057723038, 0.0686054, 0.059370305, 0.04350792, 0.06711399]\n",
      "2024-08-21 10:28:03,400 : INFO : topic #8 (0.044): 0.005*\"speech\" + 0.005*\"source\" + 0.004*\"sequence\" + 0.004*\"noise\" + 0.004*\"signal\" + 0.004*\"mixture\" + 0.004*\"recognition\" + 0.003*\"component\" + 0.003*\"language\" + 0.003*\"recurrent\"\n",
      "2024-08-21 10:28:03,400 : INFO : topic #1 (0.056): 0.008*\"layer\" + 0.008*\"hidden\" + 0.005*\"neuron\" + 0.005*\"hidden_unit\" + 0.005*\"net\" + 0.004*\"control\" + 0.003*\"region\" + 0.003*\"noise\" + 0.003*\"connection\" + 0.003*\"recognition\"\n",
      "2024-08-21 10:28:03,401 : INFO : topic #9 (0.067): 0.008*\"neuron\" + 0.008*\"cell\" + 0.006*\"response\" + 0.006*\"visual\" + 0.005*\"signal\" + 0.005*\"stimulus\" + 0.004*\"layer\" + 0.004*\"field\" + 0.003*\"activity\" + 0.003*\"frequency\"\n",
      "2024-08-21 10:28:03,401 : INFO : topic #6 (0.069): 0.004*\"classifier\" + 0.003*\"generalization\" + 0.003*\"hidden\" + 0.003*\"class\" + 0.003*\"neuron\" + 0.003*\"tree\" + 0.003*\"optimal\" + 0.003*\"prediction\" + 0.003*\"ii\" + 0.003*\"gradient\"\n",
      "2024-08-21 10:28:03,401 : INFO : topic #0 (0.080): 0.005*\"recognition\" + 0.005*\"word\" + 0.005*\"class\" + 0.004*\"image\" + 0.004*\"hidden\" + 0.004*\"node\" + 0.004*\"gaussian\" + 0.003*\"mixture\" + 0.003*\"layer\" + 0.003*\"speech\"\n",
      "2024-08-21 10:28:03,402 : INFO : topic diff=0.282554, rho=0.577350\n",
      "2024-08-21 10:28:03,405 : INFO : PROGRESS: pass 2, at document #1740/1740\n",
      "2024-08-21 10:28:05,682 : INFO : optimized alpha [0.072793, 0.047821674, 0.052701175, 0.05370678, 0.053529426, 0.050428975, 0.06170475, 0.051371314, 0.038975745, 0.06061476]\n",
      "2024-08-21 10:28:05,685 : INFO : topic #8 (0.039): 0.007*\"speech\" + 0.006*\"source\" + 0.006*\"signal\" + 0.005*\"sequence\" + 0.004*\"noise\" + 0.004*\"component\" + 0.004*\"mixture\" + 0.004*\"language\" + 0.004*\"recurrent\" + 0.003*\"recognition\"\n",
      "2024-08-21 10:28:05,685 : INFO : topic #1 (0.048): 0.011*\"hidden\" + 0.010*\"layer\" + 0.007*\"hidden_unit\" + 0.006*\"net\" + 0.005*\"control\" + 0.004*\"neuron\" + 0.004*\"region\" + 0.003*\"connection\" + 0.003*\"trained\" + 0.003*\"back\"\n",
      "2024-08-21 10:28:05,686 : INFO : topic #9 (0.061): 0.011*\"neuron\" + 0.010*\"cell\" + 0.007*\"response\" + 0.006*\"visual\" + 0.006*\"stimulus\" + 0.006*\"signal\" + 0.004*\"activity\" + 0.004*\"spike\" + 0.004*\"layer\" + 0.004*\"frequency\"\n",
      "2024-08-21 10:28:05,686 : INFO : topic #6 (0.062): 0.004*\"generalization\" + 0.004*\"classifier\" + 0.004*\"class\" + 0.003*\"prediction\" + 0.003*\"optimal\" + 0.003*\"hidden\" + 0.003*\"tree\" + 0.003*\"training_set\" + 0.003*\"bound\" + 0.003*\"gradient\"\n",
      "2024-08-21 10:28:05,686 : INFO : topic #0 (0.073): 0.006*\"recognition\" + 0.005*\"word\" + 0.005*\"class\" + 0.004*\"gaussian\" + 0.004*\"image\" + 0.004*\"hidden\" + 0.004*\"mixture\" + 0.004*\"node\" + 0.003*\"speech\" + 0.003*\"classification\"\n",
      "2024-08-21 10:28:05,687 : INFO : topic diff=0.252625, rho=0.500000\n",
      "2024-08-21 10:28:05,690 : INFO : PROGRESS: pass 3, at document #1740/1740\n",
      "2024-08-21 10:28:08,242 : INFO : optimized alpha [0.06784847, 0.04419042, 0.04637412, 0.048512477, 0.049947724, 0.0458291, 0.058109242, 0.047306877, 0.036299046, 0.056432635]\n",
      "2024-08-21 10:28:08,245 : INFO : topic #8 (0.036): 0.008*\"speech\" + 0.007*\"signal\" + 0.007*\"source\" + 0.006*\"sequence\" + 0.005*\"noise\" + 0.005*\"component\" + 0.004*\"language\" + 0.004*\"recurrent\" + 0.004*\"mixture\" + 0.003*\"matrix\"\n",
      "2024-08-21 10:28:08,246 : INFO : topic #1 (0.044): 0.014*\"hidden\" + 0.011*\"layer\" + 0.008*\"hidden_unit\" + 0.008*\"net\" + 0.005*\"control\" + 0.004*\"trained\" + 0.004*\"back\" + 0.004*\"region\" + 0.004*\"connection\" + 0.003*\"architecture\"\n",
      "2024-08-21 10:28:08,246 : INFO : topic #9 (0.056): 0.012*\"neuron\" + 0.011*\"cell\" + 0.007*\"response\" + 0.006*\"stimulus\" + 0.006*\"visual\" + 0.006*\"signal\" + 0.005*\"activity\" + 0.005*\"spike\" + 0.004*\"frequency\" + 0.004*\"field\"\n",
      "2024-08-21 10:28:08,246 : INFO : topic #6 (0.058): 0.005*\"generalization\" + 0.004*\"classifier\" + 0.004*\"prediction\" + 0.004*\"class\" + 0.003*\"optimal\" + 0.003*\"bound\" + 0.003*\"tree\" + 0.003*\"training_set\" + 0.003*\"hidden\" + 0.003*\"gradient\"\n",
      "2024-08-21 10:28:08,247 : INFO : topic #0 (0.068): 0.006*\"recognition\" + 0.006*\"word\" + 0.005*\"gaussian\" + 0.005*\"class\" + 0.004*\"mixture\" + 0.004*\"hidden\" + 0.004*\"image\" + 0.004*\"node\" + 0.004*\"likelihood\" + 0.003*\"classification\"\n",
      "2024-08-21 10:28:08,247 : INFO : topic diff=0.231254, rho=0.447214\n",
      "2024-08-21 10:28:08,250 : INFO : PROGRESS: pass 4, at document #1740/1740\n",
      "2024-08-21 10:28:10,079 : INFO : optimized alpha [0.064170465, 0.04251433, 0.0426161, 0.045125324, 0.047691107, 0.042620175, 0.056009915, 0.044773072, 0.034585927, 0.053537138]\n",
      "2024-08-21 10:28:10,082 : INFO : topic #8 (0.035): 0.009*\"speech\" + 0.008*\"signal\" + 0.008*\"source\" + 0.007*\"sequence\" + 0.005*\"noise\" + 0.005*\"component\" + 0.005*\"language\" + 0.004*\"recurrent\" + 0.004*\"mixture\" + 0.004*\"matrix\"\n",
      "2024-08-21 10:28:10,083 : INFO : topic #1 (0.043): 0.015*\"hidden\" + 0.012*\"layer\" + 0.009*\"hidden_unit\" + 0.009*\"net\" + 0.005*\"control\" + 0.005*\"trained\" + 0.004*\"back\" + 0.004*\"architecture\" + 0.004*\"connection\" + 0.004*\"hidden_layer\"\n",
      "2024-08-21 10:28:10,083 : INFO : topic #9 (0.054): 0.013*\"neuron\" + 0.012*\"cell\" + 0.008*\"response\" + 0.007*\"stimulus\" + 0.006*\"visual\" + 0.005*\"signal\" + 0.005*\"activity\" + 0.005*\"spike\" + 0.004*\"frequency\" + 0.004*\"cortex\"\n",
      "2024-08-21 10:28:10,083 : INFO : topic #6 (0.056): 0.005*\"generalization\" + 0.004*\"classifier\" + 0.004*\"prediction\" + 0.004*\"class\" + 0.004*\"bound\" + 0.004*\"optimal\" + 0.003*\"tree\" + 0.003*\"training_set\" + 0.003*\"approximation\" + 0.003*\"hidden\"\n",
      "2024-08-21 10:28:10,084 : INFO : topic #0 (0.064): 0.007*\"recognition\" + 0.006*\"word\" + 0.005*\"gaussian\" + 0.005*\"class\" + 0.005*\"mixture\" + 0.004*\"hidden\" + 0.004*\"likelihood\" + 0.004*\"image\" + 0.004*\"node\" + 0.003*\"classification\"\n",
      "2024-08-21 10:28:10,084 : INFO : topic diff=0.218164, rho=0.408248\n",
      "2024-08-21 10:28:10,087 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=8623, num_topics=10, decay=0.5, chunksize=2000> in 15.12s', 'datetime': '2024-08-21T10:28:10.087380', 'gensim': '4.3.3', 'python': '3.11.9 (main, Jul  9 2024, 11:54:13) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.2520772457122803\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5knApWshObLO"
   },
   "source": "Let's see what we got. We are interested in part of the Phi matrix – the probabilities of words in topics. The NeurIPS collection is entirely dedicated to machine learning. It's difficult to evaluate the topics, though some interpretability can be traced."
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJVQU_1rP2f2",
    "outputId": "a2e11a73-e3ff-4313-b926-7de2990f70fb",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:28:46.004706Z",
     "start_time": "2024-08-21T07:28:45.984864Z"
    }
   },
   "source": [
    "for position in range(10):\n",
    "    row = []\n",
    "    for topic in range(10):\n",
    "        row.append(model.show_topic(topic)[position][0].center(11, ' '))\n",
    "    print(''.join(row))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognition   hidden     class      layer      memory     action  generalization   image      speech     neuron  \n",
      "    word      layer      sample      chip      neuron    control   classifier   object     signal      cell   \n",
      "  gaussian hidden_unitclassification   neuron     matrix     policy   prediction   motion     source    response \n",
      "   class       net      distance     net      dynamic      rule      class      visual    sequence   stimulus \n",
      "  mixture    control     expert     analog    solution reinforcement   bound    direction    noise      visual  \n",
      "   hidden    trained     memory      bit        node     optimal    optimal      cell    component    signal  \n",
      " likelihood    back      neuron      node     circuit    dynamic      tree      field     language   activity \n",
      "   image   architecture    rule      noise       rule   reinforcement_learningtraining_set    view    recurrent    spike   \n",
      "    node    connection  instance   circuit    hopfield  controllerapproximation   pixel     mixture   frequency \n",
      "classificationhidden_layer   search   connection attractor    reward     hidden   component    matrix     cortex  \n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLIuGnNpQpyP",
    "outputId": "8f33e7e4-9250-4898-cebe-db4e1a60c4ec",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:28:53.646652Z",
     "start_time": "2024-08-21T07:28:53.561520Z"
    }
   },
   "source": [
    "top_topics = model.top_topics(corpus)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 10:28:53,602 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ-5eVhBQrsQ",
    "outputId": "8d47a367-ccf1-4569-be41-f7ec6c372d4a",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:28:54.301444Z",
     "start_time": "2024-08-21T07:28:54.297261Z"
    }
   },
   "source": [
    "top_topics[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0.024797827, 'image'),\n",
       "  (0.009840463, 'object'),\n",
       "  (0.0076078526, 'motion'),\n",
       "  (0.006597549, 'visual'),\n",
       "  (0.0063050683, 'direction'),\n",
       "  (0.0057046246, 'cell'),\n",
       "  (0.004833315, 'field'),\n",
       "  (0.004271411, 'view'),\n",
       "  (0.0041967086, 'pixel'),\n",
       "  (0.0035704765, 'component'),\n",
       "  (0.0035410635, 'recognition'),\n",
       "  (0.0035079652, 'spatial'),\n",
       "  (0.0032911452, 'filter'),\n",
       "  (0.003238161, 'vision'),\n",
       "  (0.0029603366, 'response'),\n",
       "  (0.0028917522, 'location'),\n",
       "  (0.0028470703, 'scene'),\n",
       "  (0.0028185875, 'region'),\n",
       "  (0.0027665556, 'position'),\n",
       "  (0.0027341978, 'scale')],\n",
       " -0.9543598323604014)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-2vzw2RSGFA",
    "outputId": "b4f7c699-b384-430f-83c5-953fe591bf0f",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:29:05.061036Z",
     "start_time": "2024-08-21T07:29:05.056325Z"
    }
   },
   "source": [
    "model.inference([corpus[0]])[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4170547e-02, 4.2514332e-02, 3.2069064e+02, 9.7954498e+01,\n",
       "        2.4274385e+02, 1.3815321e+02, 5.6009926e-02, 4.4773072e-02,\n",
       "        3.4585927e-02, 5.3537142e-02]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X-HTzPGxs3F"
   },
   "source": [
    "### Perplexity evaluation\n",
    "We want to assess the model with something more convincing than just looking at topic profiles and document profiles. This is necessary for the possibility of comparing different models, for example, those obtained with different run parameters. Let's learn to measure **perplexity**. The function `model.state.get_lambda` returns the unnormalized $\\Phi$ matrix, and `model.inference` estimates the unnormalized $\\Theta$ matrix for a list of documents.\n",
    "\n",
    "We iterate through the collection and calculate perplexity using the formula. The lower the perplexity, the better."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8gVV_PFRxmIP",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:35:09.048759Z",
     "start_time": "2024-08-21T07:35:09.045343Z"
    }
   },
   "source": [
    "def perplexity(model, corpus):\n",
    "    corpus_length = 0\n",
    "    log_likelihood = 0\n",
    "    topic_profiles = model.state.get_lambda() / np.sum(model.state.get_lambda(), axis=1)[:, np.newaxis]\n",
    "    for document in corpus:\n",
    "        gamma, _ = model.inference([document])\n",
    "        document_profile = gamma / np.sum(gamma)\n",
    "        for term_id, term_count in document:\n",
    "            corpus_length += term_count\n",
    "            term_probability = np.dot(document_profile, topic_profiles[:, term_id])\n",
    "            log_likelihood += term_count * log(term_probability.item())\n",
    "    perplexity = np.exp(-log_likelihood / corpus_length)\n",
    "    return perplexity"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iN6amrvHTDKt",
    "outputId": "a268a642-2efd-4217-b899-cb3abeac1f31",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:35:12.898155Z",
     "start_time": "2024-08-21T07:35:10.356568Z"
    }
   },
   "source": [
    "print('Perplexity: {}'.format(perplexity(model, corpus)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2753.1989393082135\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5_Sqil0TPYD",
    "outputId": "df2374d5-d768-4e5e-f741-15eb8ed0f0ea"
   },
   "source": [
    "model_5 = models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=5,\n",
    "    passes=epochs,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "model_20 = models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=20,\n",
    "    passes=epochs,\n",
    "    eval_every=eval_every\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYR1kXEFTWMg",
    "outputId": "2463aa3f-b0d7-4980-dcdf-43e7f62e41c7",
    "ExecuteTime": {
     "end_time": "2024-08-21T07:35:34.115201Z",
     "start_time": "2024-08-21T07:35:23.210693Z"
    }
   },
   "source": [
    "print('Perplexity 5: {}'.format(perplexity(model_5, corpus)))\n",
    "print('Perplexity 20: {}'.format(perplexity(model_20, corpus)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 5: 3120.487941603998\n",
      "Perplexity 20: 2528.991355165502\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5yh-RhCxwcT"
   },
   "source": [
    "### Word2Vec model\n",
    "Word2Vec is one of the fundamental neural network models of the \"pre-transformer\" era (2013-2018). The essence of the model is to build a mapping of words into an $N$-dimensional space (embeddings) with certain characteristics. Two words have more similar embeddings the more similar the contexts in which they are used.\n",
    "\n",
    "In the `gensim` library, two methods for building word2vec are implemented:\n",
    "  - Skip-grams (SG)\n",
    "  - Continuous-bag-of-words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYtz7rDrVyKS"
   },
   "source": [
    "## Demo\n",
    "For the demonstration, let's take a pre-trained model trained on the Google News dataset, containing approximately 3 million English words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTcMvyG1V1wH",
    "outputId": "e1aac14d-a61b-4876-84e4-7a751e10ddaf"
   },
   "source": [
    "# download the model ~1.6GB \n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gUr1QvQWCg_",
    "outputId": "5fdde298-1dfe-4067-b132-b95e91e7c6da",
    "ExecuteTime": {
     "end_time": "2024-08-21T11:58:09.737824Z",
     "start_time": "2024-08-21T11:58:09.735732Z"
    }
   },
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key )} is {word}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewckl1qAX7Cp",
    "outputId": "f7c5715c-3273-4933-f98b-ba68ca0616fd",
    "ExecuteTime": {
     "end_time": "2024-08-21T12:05:43.289732Z",
     "start_time": "2024-08-21T12:05:43.287358Z"
    }
   },
   "source": [
    "vec_king = wv['king']\n",
    "print(vec_king[:10])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12597656  0.02978516  0.00860596  0.13964844 -0.02563477 -0.03613281\n",
      "  0.11181641 -0.19824219  0.05126953  0.36328125]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the model, you can compute the distances between words."
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "359E_KR7YcQG",
    "outputId": "1cfcd100-166f-4efe-94aa-f7a0c0c21817",
    "ExecuteTime": {
     "end_time": "2024-08-21T13:08:13.809689Z",
     "start_time": "2024-08-21T13:08:13.807009Z"
    }
   },
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofdJmw4mYqKz"
   },
   "source": "You can also find the most similar words to a given one."
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "OJ1Gj7JMYtvp",
    "outputId": "c4ebce20-8cb3-41cb-bcc4-0cc2617b037d",
    "ExecuteTime": {
     "end_time": "2024-08-21T13:17:49.667766Z",
     "start_time": "2024-08-21T13:17:48.888549Z"
    }
   },
   "source": [
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763689756393433), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565719485282898)]\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:17:53.372152Z",
     "start_time": "2024-08-21T13:17:53.303241Z"
    }
   },
   "source": [
    "vec_example = wv['king'] - wv['man'] + wv['woman']\n",
    "\n",
    "similars = wv.most_similar(positive=[vec_example])\n",
    "print(similars)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', 0.8449392318725586), ('queen', 0.7300516366958618), ('monarch', 0.6454660296440125), ('princess', 0.6156251430511475), ('crown_prince', 0.5818676948547363), ('prince', 0.5777117609977722), ('kings', 0.5613663792610168), ('sultan', 0.5376776456832886), ('Queen_Consort', 0.5344247817993164), ('queens', 0.5289887189865112)]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:18:39.452818Z",
     "start_time": "2024-08-21T13:18:39.374934Z"
    }
   },
   "source": [
    "vec_example = wv['programmer'] - wv['man'] + wv['woman'] \n",
    "\n",
    "similars = wv.most_similar(positive=[vec_example])\n",
    "print(similars)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('programmer', 0.885962188243866), ('programmers', 0.6040860414505005), ('computer_programmer', 0.5623369216918945), ('coder', 0.5616979598999023), ('Programmer', 0.5576066374778748), ('programer', 0.5161396265029907), ('graphic_designer', 0.5139066576957703), ('coders', 0.48765403032302856), ('designer', 0.4822673797607422), ('librarian', 0.4649229943752289)]\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
