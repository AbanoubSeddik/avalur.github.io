{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G40l9lO2ObLr"
   },
   "source": [
    "## Homework: linear models theory (10 points max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the proposed problems. Each task must be thoroughly substantiated, tasks without\n",
    "justification are not counted. Solutions are written in a free form, however, so that the reviewer can understand. If the reviewer couldn't understand the solution to some problem,\n",
    "it is automatically not considered. If any external sources are used, they must be indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Problem 1 (1 point). Cross validation, LOO, k-fold.\n",
    " \n",
    " Explain whether it is worth using `leave-one-out-CV` estimation or `k-fold-CV` with a small $k$ when:\n",
    "\n",
    " * the training sample contains very few objects\n",
    " * the training sample contains a very large number of objects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX6sPkky9jS6"
   },
   "source": [
    "### Problem 2 (1 point). Logistic regression, deriving loss function.\n",
    "\n",
    "Consider a sample of objects $X = \\{x_1,...,x_\\ell\\}$ and their target labels $Y = \\{y_1,...,y_\\ell\\}$, where $y_i \\in \\{0, 1\\}$. Suppose we want to train a linear classifier:\n",
    "\n",
    "$$Q(w, X^\\ell) = \\sum\\limits_{i=1}^\\ell\\mathcal{L}(y_i\\left<w,x_i\\right>) \\rightarrow \\min\\limits_w$$\n",
    "\n",
    "where $w$ are the weights of the linear model, $\\mathcal{L}(y, z)$ is some smooth loss function.\n",
    "\n",
    "Since we are solving the binary classification problem, we will train the classifier to predict the probabilities of an object belonging to class $1$, i.e., solve the logistic regression problem. To measure the quality of such a classifier, the likelihood $P(Y|X)$ of the target labels $Y$ is usually used given the objects $X$ according to the predicted distributions $p$ - the higher the likelihood, the more accurate the classifier. For computational convenience, the negative log-likelihood, also known as LogLoss (Logarithmic Loss), is usually used. Let's assume that the object-answer pairs $(x_i,y_i)$ are independent of each other for different $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UX9noDn-jcv"
   },
   "source": [
    "#### 1. (0.5 points) \n",
    "Show that:\n",
    "\n",
    "$$\\text{LogLoss} =  -\\text{LogLikelihood} = - \\log(P(Y|X)) = - \\sum\\limits_{i=1}^\\ell (y_i \\log \\tilde y_i + (1-y_i) \\log (1-\\tilde y_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAZJvhZVA5-7"
   },
   "source": [
    "#### 2. (0.5 points) \n",
    "To make the classifier return numbers from the interval $[0,1]$, set\n",
    "$$\n",
    "p(y_i=1|x_i) = \\sigma \\left ( \\left< w, x_i \\right> \\right) =\n",
    "\\frac{1}{1 + \\exp{\\left(- \\left< w, x_i \\right> \\right)}}\n",
    "$$\n",
    "\n",
    "the sigmoid function increases monotonically, so the greater the dot product, the greater the probability of a positive class will be predicted for the object.\n",
    "\n",
    "Substitute the transformed response of the linear model into the logarithm of the likelihood. What loss function have we arrived at? (Note that the function is usually written for classes $\\{-1, 1\\}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG4d_zxADKDF"
   },
   "source": [
    "### Problem 3 (1.5 points). Logistic regression, solving the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA5GvcRcDY52"
   },
   "source": [
    "#### 1. (0.8 points)\n",
    "\n",
    "Prove that in the case of a linearly separable sample, there is no parameter vector (weights) that would maximize the likelihood of the probabilistic model of logistic regression in a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VBN5jiVE32G"
   },
   "source": [
    "#### 2. (0.4 points)\n",
    "\n",
    "Propose how the probabilistic model can be modified so that the optimum is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C4UiKK3HV94"
   },
   "source": [
    "#### 3. (0.3 points)\n",
    "Write out the formulas for recalculating parameter values when optimizing using gradient descent for the standard logistic regression model and the proposed modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtrnMSYXKHRS"
   },
   "source": [
    "### Problem 4 (1 point). Multinomial regression\n",
    "\n",
    "In the case of multiclass classification, logistic regression can be generalized: let there be a weight vector $w_k$ for each class $k$. Then the probability of belonging to class $k$ will be written as follows:\n",
    "\n",
    "$$\n",
    "P(y=k | x, W) = \\frac{e^{\\langle w_k, x\\rangle}}{\\sum\\limits_{j=1}^K e^{\\langle w_j, x\\rangle}}\n",
    "$$\n",
    "\n",
    "Then the optimized function will take the form:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{sm}(W) = -\\sum_{i=1}^N \\sum_{k=1}^K [y_i=k]\\ln P(y_i=k | x_i, W),~\\text{where}~[y_i=k]=\\begin{cases}\n",
    "1, y_i=k,\\\\\n",
    "0, \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Let the number of classes $K=2$. For simplicity, let's assume that the sample is linearly inseparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_Rd-dJWKm8r"
   },
   "source": [
    "#### 1. (0.5 points)\n",
    "Is the solution to the problem unique? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXe4eM0JL0xL"
   },
   "source": [
    "#### 2. (0.5 points)\n",
    "Show that the predicted probability distributions on the classes in the case of logistic and multinomial regressions will coincide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2a3lm4KMFGl"
   },
   "source": [
    "### Problem 5 (1.5 points). Decision Trees, Constant Prediction, Loss Functions\n",
    "\n",
    "Suppose, when building a decision tree, some leaf got $N$ objects $x_1, ... , x_N$ with labels $y_1, ... , y_N$.\n",
    "The prediction in each leaf of the tree is a constant. Find what value $\\tilde y$ this leaf should predict to minimize the following loss functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBcO-ZNpMSYY"
   },
   "source": [
    "#### 1. (0.5 points)\n",
    "Mean Squared Error for the regression task:\n",
    "        \\begin{equation}Q=\\frac{1}{N}\\sum_{i=1}^N (y_i - \\tilde y)^2 \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrKaxpXmNaMf"
   },
   "source": [
    "#### 2. (0.5 points) \n",
    " Mean Absolute Error for the regression task:\n",
    "        \\begin{equation}Q=\\frac{1}{N}\\sum_{i=1}^N |y_i - \\tilde y| \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnKGc3keOgF4"
   },
   "source": [
    "#### 3. (0.5 points)\n",
    " $\\text{LogLoss}$ (logarithmic losses) for classification task:\n",
    "        $$Q=-\\frac{1}{N}\\sum_{i=1}^N \\left(y_i\\log\\tilde y+(1-y_i)\\log(1-\\tilde y)\\right),\n",
    "            \\quad \\tilde y\\in[0,1], \\quad y_i \\in \\{0,1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSmKQAILP0T0"
   },
   "source": [
    " ### Problem 6 (1 point). Decision Trees, Loss Functions, Impurity Functions\n",
    "$$\n",
    " \\Phi(U) - \\frac{|U_1|}{|U|}\\Phi(U_1) - \\frac{|U_2|}{|U|}\\Phi(U_2) \\to \\max\n",
    "$$\n",
    "This expression is used in the lecture to set the criterion by which a fork in a decision tree node occurs. Let's delve into this.\n",
    "\n",
    "Impurity function $\\Phi(U)$ is used to measure the degree of heterogeneity of the target labels $y_1,\\dots, y_\\ell$ for a set of objects $U$ of size $\\ell$. For example, when training a decision tree in the current leaf, such a splitting of the set of objects $U$ into two disjoint sets $U_1$ and $U_2$ is chosen so that the impurity function $\\Phi(U)$ of the original set $U$ would as much as possible exceed the normalized impurity function in the new leaves $\\frac{|U_1|}{|U|}\\Phi(U_1) + \\frac{|U_2|}{|U|}\\Phi(U_2)$. Hence, we need to choose a partition that solves the task\n",
    "$$\n",
    " \\Phi(U) - \\frac{|U_1|}{|U|}\\Phi(U_1) - \\frac{|U_2|}{|U|}\\Phi(U_2) \\to \\max\n",
    "$$\n",
    "The resulting difference is called Gain, and it shows how much it was possible to reduce the \"uncertainty\" from splitting a leaf into two new ones. \n",
    "\n",
    "According to one of the possible definitions, the impurity function is the value of the error functional $Q = \\frac{1}{\\ell}\\sum\\limits_{i=1}^\\ell \\mathcal{L}(y_i, \\tilde{y})$ in the leaf with a set of objects $U$ with a constant prediction $\\tilde{y}$, optimal for $Q$ (see problem 7):\n",
    "    $$\n",
    "        \\Phi(U) = \\frac{1}{\\ell}\\sum\\limits_{i=1}^\\ell \\mathcal L (y_i, \\tilde y)\n",
    "    $$\n",
    "It is clear that each splitting criterion corresponds to its own impurity function $\\Phi(U)$, and at the basis of each $\\Phi(U)$ lies some loss function. Let's figure out where the different splitting criteria come from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZNypaDVQDYZ"
   },
   "source": [
    "#### 1. (0.5 points)\n",
    "Show that for quadratic losses $\\mathcal L (y_i, \\tilde y) = (y_i - \\tilde y)^2$ in the regression problem $y_i \\in \\mathbb{R}$ the impurity function $\\Phi(U)$ equals the sample variance of the target labels of the objects that got into the tree leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeWp8iIxR96A"
   },
   "source": [
    "#### 2. (0.5 points)\n",
    " Show that for the loss function $\\text{Logloss}$  $\\mathcal L (y_i, \\tilde y) =-y_i\\log(\\tilde y) - (1-y_i)\\log(1 - \\tilde y)$ in the classification problem $y_i \\in \\{0,1\\}$ the impurity function $\\Phi(U)$ corresponds to the entropy splitting criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvAw2PMSTrkw"
   },
   "source": [
    "### Problem 7 (1 point). Decision Trees, Gini Index\n",
    "\n",
    "Suppose a decision tree has been built for a multi-class classification task. Let's consider the leaf of the tree with number $m$ and the objects $R_m$ that fell into it. Denote as $p_{mk}$ the fraction of objects of class $k$ in the leaf $m$. The *Gini index* of this leaf is called the quantity\n",
    "$$\\sum_{k = 1}^{K} p_{mk} (1 - p_{mk}),$$\n",
    "where $K$ is the total number of classes. The Gini index usually serves as a measure of how well a particular class is singled out in a given leaf (see impurity function in the previous problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39o2PqSqT9U-"
   },
   "source": [
    "#### 1. (0.5 points)\n",
    "Let's assign to the leaf $m$ a classification algorithm $a(x)$, which predicts the class randomly, and the class $k$ is chosen with probability $p_{mk}$. Show that the expected value of the error rate of this algorithm on objects from $R_m$ is equal to the Gini index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2_WiDXWU5Mu"
   },
   "source": [
    "#### 2. (0.5 points)\n",
    "Let's call the *variance of class $k$* the variance of the sample $\\{ [y_i = k]:\\ x_i \\in R_m\\}$, where $y_i$ is the class of object $x_i$, $[f]$ is a truth indicator of the expression $f$, equal to 1 if $f$ is true, and zero otherwise, and $R_m$ is the set of objects in the leaf. \n",
    "\n",
    "Show that the sum of the variances of all classes in the given leaf equals its Gini index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6-ynDEVYCF4"
   },
   "source": [
    "### Problem 8 (2 points). Binary Decision Trees, MSE\n",
    "\n",
    "Suggest an algorithm for building an *optimal* binary decision tree for a regression task on $\\ell$ objects in an $n$-dimensional space with asymptotic complexity $O(n \\ell \\log \\ell)$. The predicates to be considered are threshold rules (the most common case in practice). For simplicity, you can assume that the resulting tree is close to balanced (i.e., its depth is of the order of $O(\\log \\ell)$) and the Mean Squared Error (MSE) is used as the error function:\n",
    "\t$$Q=\\frac{1}{\\ell}\\sum_{i=1}^\\ell (y_i - \\tilde y_i)^2$$\n",
    "By optimality in this problem is meant that at each node of the tree, an optimal division into two subtrees is made in terms of MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "Practice5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
